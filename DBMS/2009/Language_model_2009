Language model
A statistical language model assigns a probability to a sequence of m words 



P
(

w

1


,
…
,

w

m


)


{\displaystyle P(w_{1},\ldots ,w_{m})}

 by means of a probability distribution.
Language modeling is used in many natural language processing applications such as speech recognition, machine translation, part-of-speech tagging, parsing and information retrieval.
In speech recognition and in data compression, such a model tries to capture the properties of a language, and to predict the next word in a speech sequence.
When used in information retrieval, a language model is associated with a document in a collection. With query Q as input, retrieved documents are ranked based on the probability that the document's language model would generate the terms of the query, P(Q|Md).
Estimating the probability of sequences can become difficult in corpora, in which phrases or sentences can be arbitrarily long and hence some sequences are not observed during training of the language model (data sparseness problem of overfitting). For that reason these models are often approximated using smoothed N-gram models.

Contents
1 N-gram models

1.1 Example


1.1 Example
2 See also
3 References

N-gram models
In an n-gram model, the probability 



P
(

w

1


,
…
,

w

m


)


{\displaystyle P(w_{1},\ldots ,w_{m})}

 of observing the sentence w1,...,wm is approximated as




P
(

w

1


,
…
,

w

m


)
=

∏

i
=
1


m


P
(

w

i



|


w

1


,
…
,

w

i
−
1


)
≈

∏

i
=
1


m


P
(

w

i



|


w

i
−
(
n
−
1
)


,
…
,

w

i
−
1


)


{\displaystyle P(w_{1},\ldots ,w_{m})=\prod _{i=1}^{m}P(w_{i}|w_{1},\ldots ,w_{i-1})\approx \prod _{i=1}^{m}P(w_{i}|w_{i-(n-1)},\ldots ,w_{i-1})}


Here, it is assumed that the probability of observing the ith word wi in the context history of the preceding i-1 words can be approximated by the probability of observing it in the shortened context history of the preceding n-1 words (nth order Markov property).
The conditional probability can be calculated from n-gram frequency counts: 



P
(

w

i



|


w

i
−
(
n
−
1
)


,
…
,

w

i
−
1


)
=



c
o
u
n
t
(

w

i
−
(
n
−
1
)


,
…
,

w

i
−
1


,

w

i


)


c
o
u
n
t
(

w

i
−
(
n
−
1
)


,
…
,

w

i
−
1


)





{\displaystyle P(w_{i}|w_{i-(n-1)},\ldots ,w_{i-1})={\frac {count(w_{i-(n-1)},\ldots ,w_{i-1},w_{i})}{count(w_{i-(n-1)},\ldots ,w_{i-1})}}}



The words bigram and trigram language model denote n-gram language models with n=2 and n=3, respectively.
Example
In a bigram (n=2) language model, the probability of the sentence I saw the red house is approximated as




P
(
I
,
s
a
w
,
t
h
e
,
r
e
d
,
h
o
u
s
e
)
≈
P
(
I

|

<
s
>
)
P
(
s
a
w

|

I
)
P
(
t
h
e

|

s
a
w
)
P
(
r
e
d

|

t
h
e
)
P
(
h
o
u
s
e

|

r
e
d
)


{\displaystyle P(I,saw,the,red,house)\approx P(I|<s>)P(saw|I)P(the|saw)P(red|the)P(house|red)}


whereas in a trigram (n=3) language model, the approximation is




P
(
I
,
s
a
w
,
t
h
e
,
r
e
d
,
h
o
u
s
e
)
≈
P
(
I

|

<
s
>
,
<
s
>
)
P
(
s
a
w

|

<
s
>
,
I
)
P
(
t
h
e

|

I
,
s
a
w
)
P
(
r
e
d

|

s
a
w
,
t
h
e
)
P
(
h
o
u
s
e

|

t
h
e
,
r
e
d
)


{\displaystyle P(I,saw,the,red,house)\approx P(I|<s>,<s>)P(saw|<s>,I)P(the|I,saw)P(red|saw,the)P(house|the,red)}


Note, that the context of the first n-1 ngrams is filled start-of-sentence markers, typically denoted <s>.
See also
Factored language model
Katz's back-off model
References
J M Ponte and W B Croft (1998). "A Language Modeling Approach to Information Retrieval". Research and Development in Information Retrieval. pp. 275–281. 
F Song and W B Croft (1999). "A General Language Model for Information Retrieval". Research and Development in Information Retrieval. pp. 279–280. 
